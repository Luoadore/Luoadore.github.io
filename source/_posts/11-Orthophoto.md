---
title: Orthophoto
date: 2020-11-13 22:21:00
tags: 5 Minutes with Cyrill
---

# Orthophoto

How can you measure the distance in an image? Measure distances in image and know that those distances are corresponded to the distance in the real world. This is something which typically dose not work with your regular camera with a perspective camera because this camera take the points in the 3D world and project them onto a 2D image plane with a central projection or an approximation of the central projection. That project doesn’t lead to the distance in the image plane that corresponds to the distances in the real world. So you need to do you need to generate so-called **Orthophoto**. In an orthophoto you can envision this as the picture which have been taken very very far away from an object like sitting in space somewhere on a satellite and take an image of universe. Because this set up all the **rays from your image plane towards the object are more or less parallel**. In the more or less parallel, then you actually generate an image where you can perform those measurements in an image. In practice however, you can’t generate those images or it’s very tricky.

So what do you need to do in order to get a orthophoto? The good news is you can compute an orthophoto given an real world image. You need a bit more information than just the image you also need to have known the geometry of the object you’re picturing. You need to know where the image has been taken from and what your camera parameters are (**camera image surface model extrinsics, intrinsics**). But with this information, you can compute a so-called orthophoto.

Let’s see how that works. We have a picture taken from this colored surface for example and we want to generating orthophotos from that surface. We generate a new so-called virtual image so an image that doesn’t exist in reality but that I want to compute on my orthophoto. I put it as a grid underneath my surface. Then for every pixels in this new virtual image I move up to the surface here along the Z direction until I Intersect with that surface and this gives me a point in the 3d world. And I take this point and projecting this point into my camera image using my standard x equals px equation we use to transform the 3D points with the prediction matrix into the image point and then what I'm doing I'm just taking the color value or the intensity value at this pixel location, copy it and store it at the corresponding location in my orthophoto. I'm simply repeating this process over and over again for all those pixel locations in my orthophoto which will generate me a new photo where I can measure the distances in this new image and they correspond to the distances in the 2D world.

![orthophoto](orthophoto.png)

That's the way you can generate orthophoto at least in its most simple form and what you need to do is you need to have your surface model, where your image has been taken, your camera parameters. But you can generate this orthophoto which allows you then perform measurement in those images. This important when building maps if you for example use google maps and take the image overlay of the area image with a map. That is something which is very close on approximation of an orthophoto. So you can measure the distances of image and those distances correspond to the distances in the real world.